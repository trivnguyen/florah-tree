{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edaa81d",
   "metadata": {},
   "source": [
    "# FLORAH Tree Generator - Training Tutorial\n",
    "\n",
    "This tutorial will guide you through training the FLORAH Tree Generator model step by step. The FLORAH Tree Generator is a machine learning model that generates merger trees for dark matter halos in cosmological simulations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The training process involves:\n",
    "1. Setting up the environment and dependencies\n",
    "2. Preparing your dataset\n",
    "3. Configuring the model parameters\n",
    "4. Running the training\n",
    "5. Monitoring training progress\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- GPU with CUDA support (recommended)\n",
    "- Sufficient disk space for datasets and model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75cd0e",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cae7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch pytorch-lightning tensorboard ml-collections absl-py torch-geometric pyyaml numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from ml_collections import config_dict\n",
    "import numpy as np\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA devices: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53125b",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation\n",
    "\n",
    "The model requires processed merger tree data. Your dataset should be organized as follows:\n",
    "\n",
    "```\n",
    "datasets/\n",
    "├── processed/\n",
    "│   └── your_dataset_name/\n",
    "│       ├── file_0.pkl\n",
    "│       ├── file_1.pkl\n",
    "│       └── ...\n",
    "```\n",
    "\n",
    "Each pickle file should contain merger tree data in the expected format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbcaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset directory exists and list available datasets\n",
    "dataset_root = \"./datasets/processed/\"\n",
    "\n",
    "if os.path.exists(dataset_root):\n",
    "    datasets = [d for d in os.listdir(dataset_root) if os.path.isdir(os.path.join(dataset_root, d))]\n",
    "    print(f\"Available datasets in {dataset_root}:\")\n",
    "    for dataset in datasets:\n",
    "        dataset_path = os.path.join(dataset_root, dataset)\n",
    "        num_files = len([f for f in os.listdir(dataset_path) if f.endswith('.pkl')])\n",
    "        print(f\"  - {dataset}: {num_files} files\")\n",
    "else:\n",
    "    print(f\"Dataset directory not found: {dataset_root}\")\n",
    "    print(\"Please create the dataset directory and add your processed data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92ded9",
   "metadata": {},
   "source": [
    "## Step 3: Configuration Setup\n",
    "\n",
    "Now let's create a configuration for training. We'll start with a basic configuration that you can modify based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd6d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_config(dataset_name, experiment_name):\n",
    "    \"\"\"Create a training configuration\"\"\"\n",
    "    config = config_dict.ConfigDict()\n",
    "\n",
    "    # Basic experiment settings\n",
    "    config.workdir = './training_logs'  # Where to save training logs and checkpoints\n",
    "    config.name = experiment_name\n",
    "    config.overwrite = True  # Overwrite existing experiment directory\n",
    "    config.enable_progress_bar = True\n",
    "    config.checkpoint = None  # Start from scratch\n",
    "    config.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    config.reset_optimizer = False\n",
    "\n",
    "    # Random seeds for reproducibility\n",
    "    config.seed = config_dict.ConfigDict()\n",
    "    config.seed.data = 42\n",
    "    config.seed.training = 1337\n",
    "    config.seed.inference = 9999\n",
    "\n",
    "    # Dataset configuration\n",
    "    config.data = config_dict.ConfigDict()\n",
    "    config.data.root = \"./datasets/processed/\"\n",
    "    config.data.name = dataset_name\n",
    "    config.data.num_files = 10  # Number of dataset files to use\n",
    "    config.data.index_file_start = 0\n",
    "    config.data.train_frac = 0.8  # 80% for training, 20% for validation\n",
    "    config.data.reverse_time = False\n",
    "\n",
    "    # Model architecture\n",
    "    config.model = config_dict.ConfigDict()\n",
    "    config.model.name = 'atg2'\n",
    "    config.model.d_in = 2  # Input feature dimension\n",
    "    config.model.num_classes = 3  # Number of output classes\n",
    "\n",
    "    # Encoder configuration (GRU-based)\n",
    "    config.model.encoder = config_dict.ConfigDict()\n",
    "    config.model.encoder.name = 'gru'\n",
    "    config.model.encoder.d_model = 128\n",
    "    config.model.encoder.d_out = 128\n",
    "    config.model.encoder.dim_feedforward = 128\n",
    "    config.model.encoder.num_layers = 4\n",
    "    config.model.encoder.concat = False\n",
    "\n",
    "    # Decoder configuration (GRU-based)\n",
    "    config.model.decoder = config_dict.ConfigDict()\n",
    "    config.model.decoder.name = 'gru'\n",
    "    config.model.decoder.d_model = 128\n",
    "    config.model.decoder.d_out = 128\n",
    "    config.model.decoder.dim_feedforward = 128\n",
    "    config.model.decoder.num_layers = 4\n",
    "    config.model.decoder.concat = False\n",
    "\n",
    "    # Neural Posterior Estimation (NPE) configuration\n",
    "    config.model.npe = config_dict.ConfigDict()\n",
    "    config.model.npe.hidden_sizes = [128, 128]\n",
    "    config.model.npe.num_transforms = 4\n",
    "    config.model.npe.context_embedding_sizes = None\n",
    "    config.model.npe.dropout = 0.2\n",
    "\n",
    "    # Classifier configuration\n",
    "    config.model.classifier = config_dict.ConfigDict()\n",
    "    config.model.classifier.d_context = 1\n",
    "    config.model.classifier.hidden_sizes = [128, 128]\n",
    "\n",
    "    # Optimizer configuration\n",
    "    config.optimizer = config_dict.ConfigDict()\n",
    "    config.optimizer.name = 'AdamW'\n",
    "    config.optimizer.lr = 5e-5  # Learning rate\n",
    "    config.optimizer.betas = (0.9, 0.98)\n",
    "    config.optimizer.weight_decay = 1e-4\n",
    "    config.optimizer.eps = 1e-9\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    config.scheduler = config_dict.ConfigDict()\n",
    "    config.scheduler.name = 'WarmUpCosineAnnealingLR'\n",
    "    config.scheduler.decay_steps = 100_000  # Total training steps\n",
    "    config.scheduler.warmup_steps = 5_000   # Warmup steps\n",
    "    config.scheduler.eta_min = 1e-6\n",
    "    config.scheduler.interval = 'step'\n",
    "\n",
    "    # Training configuration\n",
    "    config.training = config_dict.ConfigDict()\n",
    "    config.training.max_epochs = 100\n",
    "    config.training.max_steps = 100_000\n",
    "    config.training.train_batch_size = 32  # Adjust based on your GPU memory\n",
    "    config.training.eval_batch_size = 32\n",
    "    config.training.monitor = 'val_loss'\n",
    "    config.training.patience = 10  # Early stopping patience\n",
    "    config.training.save_top_k = 3\n",
    "    config.training.save_last_k = 3\n",
    "    config.training.gradient_clip_val = 0.5\n",
    "    config.training.num_workers = 4\n",
    "\n",
    "    # Training mode settings\n",
    "    config.training.training_mode = 'all'\n",
    "    config.training.use_sample_weight = False\n",
    "    config.training.use_desc_mass_ratio = False\n",
    "    config.training.num_branches_per_tree = 10\n",
    "\n",
    "    # Freezing configuration (which parts of the model to freeze)\n",
    "    config.training.freeze_args = config_dict.ConfigDict()\n",
    "    config.training.freeze_args.encoder = False\n",
    "    config.training.freeze_args.decoder = False\n",
    "    config.training.freeze_args.npe = False\n",
    "    config.training.freeze_args.classifier = False\n",
    "\n",
    "    return config\n",
    "\n",
    "# Create a sample configuration\n",
    "# Replace 'your_dataset_name' with your actual dataset name\n",
    "dataset_name = \"sample_dataset\"  # Change this to your dataset name\n",
    "experiment_name = \"my_first_training\"\n",
    "\n",
    "config = create_training_config(dataset_name, experiment_name)\n",
    "print(f\"Created configuration for experiment: {experiment_name}\")\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Training device: {config.accelerator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f24a0",
   "metadata": {},
   "source": [
    "## Step 4: Configuration Customization\n",
    "\n",
    "You can modify the configuration based on your specific needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize configuration based on your needs\n",
    "\n",
    "# If you have a specific dataset, update the name and number of files\n",
    "# config.data.name = \"your_actual_dataset_name\"\n",
    "# config.data.num_files = 20  # Adjust based on your dataset size\n",
    "\n",
    "# If you have limited GPU memory, reduce batch size\n",
    "# config.training.train_batch_size = 16\n",
    "# config.training.eval_batch_size = 16\n",
    "\n",
    "# If you want to train for longer\n",
    "# config.training.max_epochs = 200\n",
    "# config.training.max_steps = 200_000\n",
    "\n",
    "# If you want to adjust the model size (smaller for faster training, larger for better performance)\n",
    "# config.model.encoder.d_model = 64  # Smaller model\n",
    "# config.model.decoder.d_model = 64\n",
    "\n",
    "print(\"Configuration ready for training!\")\n",
    "print(f\"Batch size: {config.training.train_batch_size}\")\n",
    "print(f\"Max epochs: {config.training.max_epochs}\")\n",
    "print(f\"Model size: {config.model.encoder.d_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74f430",
   "metadata": {},
   "source": [
    "## Step 5: Training Function\n",
    "\n",
    "Now let's set up the training function. This is adapted from the `train_atg.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training modules\n",
    "import datasets\n",
    "from florah_tree.atg import AutoregTreeGen\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import shutil\n",
    "\n",
    "def train_model(config):\n",
    "    \"\"\"Train the FLORAH Tree Generator model\"\"\"\n",
    "\n",
    "    # Set up work directory\n",
    "    workdir = os.path.join(config.workdir, config.name)\n",
    "\n",
    "    # Handle existing directory\n",
    "    if os.path.exists(workdir):\n",
    "        if config.overwrite:\n",
    "            shutil.rmtree(workdir)\n",
    "            print(f\"Removed existing directory: {workdir}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Directory {workdir} already exists. Set overwrite=True to overwrite.\")\n",
    "\n",
    "    os.makedirs(workdir, exist_ok=True)\n",
    "    print(f\"Created training directory: {workdir}\")\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = os.path.join(workdir, 'config.yaml')\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config.to_dict(), f, default_flow_style=False)\n",
    "    print(f\"Saved configuration to: {config_path}\")\n",
    "\n",
    "    # Load dataset and prepare dataloader\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        train_loader, val_loader, norm_dict = datasets.prepare_dataloader(\n",
    "            datasets.read_dataset(\n",
    "                dataset_root=config.data.root,\n",
    "                dataset_name=config.data.name,\n",
    "                max_num_files=config.data.get(\"num_files\", 1),\n",
    "            ),\n",
    "            train_frac=config.data.train_frac,\n",
    "            train_batch_size=config.training.train_batch_size,\n",
    "            eval_batch_size=config.training.eval_batch_size,\n",
    "            use_sampler=config.training.get(\"use_sampler\", False),\n",
    "            sampler_args=config.training.get(\"sampler_args\"),\n",
    "            num_workers=config.training.get(\"num_workers\", 0),\n",
    "            seed=config.seed.data,\n",
    "            norm_dict=None,\n",
    "            reverse_time=config.data.get(\"reverse_time\", False),\n",
    "        )\n",
    "        print(f\"Dataset loaded successfully!\")\n",
    "        print(f\"Training batches: {len(train_loader)}\")\n",
    "        print(f\"Validation batches: {len(val_loader)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create the model\n",
    "    print(\"Creating model...\")\n",
    "    model = AutoregTreeGen(\n",
    "        d_in=config.model.d_in,\n",
    "        num_classes=config.model.num_classes,\n",
    "        encoder_args=config.model.encoder,\n",
    "        decoder_args=config.model.decoder,\n",
    "        npe_args=config.model.npe,\n",
    "        classifier_args=config.model.classifier,\n",
    "        optimizer_args=config.optimizer,\n",
    "        scheduler_args=config.scheduler,\n",
    "        training_args=config.training,\n",
    "        norm_dict=norm_dict,\n",
    "    )\n",
    "\n",
    "    # Count model parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model created with {total_params:,} total parameters ({trainable_params:,} trainable)\")\n",
    "\n",
    "    # Create callbacks\n",
    "    callbacks = [\n",
    "        pl.callbacks.EarlyStopping(\n",
    "            monitor=config.training.get('monitor', 'val_loss'),\n",
    "            patience=config.training.get('patience', 10),\n",
    "            mode='min',\n",
    "            verbose=True\n",
    "        ),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            filename=\"best-{epoch}-{step}-{val_loss:.4f}\",\n",
    "            monitor=config.training.get('monitor', 'val_loss'),\n",
    "            save_top_k=config.training.get('save_top_k', 1),\n",
    "            mode='min',\n",
    "            save_weights_only=False,\n",
    "            save_last=False\n",
    "        ),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            filename=\"last-{epoch}-{step}-{val_loss:.4f}\",\n",
    "            save_top_k=config.training.get('save_last_k', 1),\n",
    "            monitor='epoch',\n",
    "            mode='max',\n",
    "            save_weights_only=False,\n",
    "            save_last=False\n",
    "        ),\n",
    "        pl.callbacks.LearningRateMonitor(\"step\"),\n",
    "    ]\n",
    "\n",
    "    # Create trainer\n",
    "    train_logger = pl_loggers.TensorBoardLogger(workdir, version='')\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=workdir,\n",
    "        max_epochs=config.training.max_epochs,\n",
    "        max_steps=config.training.max_steps,\n",
    "        accelerator=config.accelerator,\n",
    "        callbacks=callbacks,\n",
    "        logger=train_logger,\n",
    "        gradient_clip_val=config.training.get('gradient_clip_val', 0),\n",
    "        enable_progress_bar=config.get(\"enable_progress_bar\", True),\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "\n",
    "    return model, trainer, train_loader, val_loader\n",
    "\n",
    "print(\"Training function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504864a",
   "metadata": {},
   "source": [
    "## Step 6: Start Training\n",
    "\n",
    "Now let's start the training process. Make sure you have your dataset ready before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf40c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running training, let's do a quick check\n",
    "dataset_path = os.path.join(config.data.root, config.data.name)\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"❌ Dataset not found at: {dataset_path}\")\n",
    "    print(\"Please make sure your dataset is properly located.\")\n",
    "    print(\"\\nDataset should be organized as:\")\n",
    "    print(f\"  {config.data.root}\")\n",
    "    print(f\"  └── {config.data.name}/\")\n",
    "    print(\"      ├── file_0.pkl\")\n",
    "    print(\"      ├── file_1.pkl\")\n",
    "    print(\"      └── ...\")\n",
    "else:\n",
    "    print(f\"✅ Dataset found at: {dataset_path}\")\n",
    "\n",
    "    # Set up training\n",
    "    try:\n",
    "        model, trainer, train_loader, val_loader = train_model(config)\n",
    "\n",
    "        if model is not None:\n",
    "            print(\"\\n🚀 Starting training...\")\n",
    "            print(f\"Training will run for up to {config.training.max_epochs} epochs or {config.training.max_steps} steps\")\n",
    "            print(f\"Early stopping patience: {config.training.patience} epochs\")\n",
    "            print(f\"Checkpoints will be saved to: {os.path.join(config.workdir, config.name)}\")\n",
    "\n",
    "            # Set random seed for reproducibility\n",
    "            pl.seed_everything(config.seed.training)\n",
    "\n",
    "            # Start training\n",
    "            trainer.fit(\n",
    "                model,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=val_loader,\n",
    "            )\n",
    "\n",
    "            print(\"\\n🎉 Training completed!\")\n",
    "            print(f\"Best model saved to: {trainer.checkpoint_callback.best_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe55c61",
   "metadata": {},
   "source": [
    "## Step 7: Monitoring Training Progress\n",
    "\n",
    "You can monitor the training progress using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c27d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TensorBoard to monitor training\n",
    "tensorboard_log_dir = os.path.join(config.workdir, config.name, \"lightning_logs\")\n",
    "\n",
    "if os.path.exists(tensorboard_log_dir):\n",
    "    print(f\"TensorBoard logs available at: {tensorboard_log_dir}\")\n",
    "    print(\"\\nTo view training progress, run in terminal:\")\n",
    "    print(f\"tensorboard --logdir {tensorboard_log_dir}\")\n",
    "    print(\"\\nThen open http://localhost:6006 in your browser\")\n",
    "\n",
    "    # You can also start TensorBoard from here (uncomment the next line)\n",
    "    # %load_ext tensorboard\n",
    "    # %tensorboard --logdir {tensorboard_log_dir}\n",
    "else:\n",
    "    print(\"No TensorBoard logs found yet. Start training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c427e0",
   "metadata": {},
   "source": [
    "## Step 8: Training Results and Next Steps\n",
    "\n",
    "After training completes, you'll find the following in your training directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd4bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training results\n",
    "training_dir = os.path.join(config.workdir, config.name)\n",
    "\n",
    "if os.path.exists(training_dir):\n",
    "    print(f\"Training directory: {training_dir}\")\n",
    "    print(\"\\nContents:\")\n",
    "    for item in os.listdir(training_dir):\n",
    "        item_path = os.path.join(training_dir, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"  📁 {item}/\")\n",
    "        else:\n",
    "            print(f\"  📄 {item}\")\n",
    "\n",
    "    # Check for checkpoints\n",
    "    checkpoint_dir = os.path.join(training_dir, \"lightning_logs\", \"checkpoints\")\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.ckpt')]\n",
    "        print(f\"\\nCheckpoints ({len(checkpoints)} found):\")\n",
    "        for ckpt in sorted(checkpoints):\n",
    "            print(f\"  🔖 {ckpt}\")\n",
    "    else:\n",
    "        print(\"\\nNo checkpoints found.\")\n",
    "else:\n",
    "    print(\"Training directory not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2666b9b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "1. **Dataset not found**: Make sure your dataset is in the correct location and format.\n",
    "\n",
    "2. **Out of memory**: Reduce batch size in the configuration:\n",
    "   ```python\n",
    "   config.training.train_batch_size = 16  # or smaller\n",
    "   config.training.eval_batch_size = 16\n",
    "   ```\n",
    "\n",
    "3. **Training too slow**: \n",
    "   - Use GPU if available\n",
    "   - Reduce model size:\n",
    "     ```python\n",
    "     config.model.encoder.d_model = 64\n",
    "     config.model.decoder.d_model = 64\n",
    "     ```\n",
    "\n",
    "4. **Training not converging**: \n",
    "   - Adjust learning rate: `config.optimizer.lr = 1e-4`\n",
    "   - Increase patience: `config.training.patience = 20`\n",
    "   - Check your data quality\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After successful training:\n",
    "\n",
    "1. **Evaluate your model**: Use validation metrics from TensorBoard\n",
    "2. **Run inference**: Use the inference tutorial with your trained model\n",
    "3. **Fine-tune**: Adjust hyperparameters and retrain if needed\n",
    "4. **Save your model**: The best checkpoint is automatically saved\n",
    "\n",
    "## Configuration Templates\n",
    "\n",
    "Here are some preset configurations for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quick_test_config():\n",
    "    \"\"\"Quick test configuration for debugging\"\"\"\n",
    "    config = create_training_config(\"your_dataset\", \"quick_test\")\n",
    "    config.training.max_epochs = 5\n",
    "    config.training.max_steps = 100\n",
    "    config.training.train_batch_size = 8\n",
    "    config.training.eval_batch_size = 8\n",
    "    config.model.encoder.d_model = 32\n",
    "    config.model.decoder.d_model = 32\n",
    "    return config\n",
    "\n",
    "def get_high_performance_config():\n",
    "    \"\"\"High performance configuration for production\"\"\"\n",
    "    config = create_training_config(\"your_dataset\", \"high_performance\")\n",
    "    config.training.max_epochs = 500\n",
    "    config.training.max_steps = 500_000\n",
    "    config.training.train_batch_size = 64\n",
    "    config.training.eval_batch_size = 64\n",
    "    config.model.encoder.d_model = 256\n",
    "    config.model.decoder.d_model = 256\n",
    "    config.model.encoder.num_layers = 6\n",
    "    config.model.decoder.num_layers = 6\n",
    "    config.optimizer.lr = 1e-5\n",
    "    return config\n",
    "\n",
    "def get_memory_efficient_config():\n",
    "    \"\"\"Memory efficient configuration for limited resources\"\"\"\n",
    "    config = create_training_config(\"your_dataset\", \"memory_efficient\")\n",
    "    config.training.train_batch_size = 8\n",
    "    config.training.eval_batch_size = 8\n",
    "    config.model.encoder.d_model = 64\n",
    "    config.model.decoder.d_model = 64\n",
    "    config.model.encoder.num_layers = 2\n",
    "    config.model.decoder.num_layers = 2\n",
    "    config.training.num_workers = 2\n",
    "    return config\n",
    "\n",
    "print(\"Configuration templates created!\")\n",
    "print(\"Use get_quick_test_config(), get_high_performance_config(), or get_memory_efficient_config()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a7701e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully learned how to train the FLORAH Tree Generator! \n",
    "\n",
    "**Summary of what you accomplished:**\n",
    "- Set up the training environment\n",
    "- Configured the model parameters\n",
    "- Prepared the dataset\n",
    "- Ran the training process\n",
    "- Learned how to monitor progress\n",
    "\n",
    "**Your trained model is now ready for inference!** \n",
    "\n",
    "Check out the inference tutorial (`tutorial_inference.ipynb`) to learn how to generate merger trees using your trained model.\n",
    "\n",
    "Good luck with your cosmological simulations! 🌌"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
