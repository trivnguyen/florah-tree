{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffc7d08",
   "metadata": {},
   "source": [
    "# FLORAH Tree Generator: Training Tutorial\n",
    "\n",
    "Welcome to the FLORAH training tutorial! This notebook provides a step-by-step guide to training your own generative model for dark matter halo merger trees. We will walk through setting up the environment, configuring the model, loading data, and running the training process.\n",
    "\n",
    "The goal is to provide a clear, modular walkthrough that separates each key stage of the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0579377",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, we need to install the necessary Python packages and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba0aaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies are assumed to be installed.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages. Uncomment the line below if you haven't installed them yet.\n",
    "# !pip install torch pytorch-lightning tensorboard ml-collections absl-py torch-geometric pyyaml numpy tqdm\n",
    "\n",
    "print(\"Dependencies are assumed to be installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40c63a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: /mnt/home/tnguyen/projects/florah/florah-tree\n",
      "PyTorch Lightning version: 2.1.3\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import shutil\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from ml_collections import config_dict\n",
    "import numpy as np\n",
    "\n",
    "# Import FLORAH-specific modules\n",
    "import datasets\n",
    "from florah_tree.atg import AutoregTreeGen\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "\n",
    "# Add the project root to the Python path to ensure modules are found\n",
    "# This assumes the notebook is run from the root of the florah-tree repository\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"PyTorch Lightning version: {pl.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbdbf52",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All aspects of the training run are controlled by a single configuration object. This makes experiments reproducible and easy to modify. Here, we define a sample configuration. You should adapt the paths and parameters for your specific setup and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9587d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration created for experiment: 'florah_tutorial_run'\n",
      "Training will run on: gpu\n",
      "Dataset to be used: vsmdpl-nprog3-dt2_6-z10\n"
     ]
    }
   ],
   "source": [
    "# Create a configuration using ml_collections.config_dict\n",
    "config = config_dict.ConfigDict()\n",
    "\n",
    "# -- Logging and Environment --\n",
    "config.workdir = './training_logs'  # Directory to save logs and checkpoints\n",
    "config.name = 'florah_tutorial_run' # A name for this specific experiment\n",
    "config.overwrite = True  # If True, deletes previous run with the same name\n",
    "config.accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# -- Seeds for reproducibility --\n",
    "config.seed = config_dict.ConfigDict()\n",
    "config.seed.data = 42\n",
    "config.seed.training = 1337\n",
    "\n",
    "# -- Dataset Configuration --\n",
    "config.data = config_dict.ConfigDict()\n",
    "config.data.root = \"./datasets/processed/\" # Path to your processed datasets\n",
    "config.data.name = \"vsmdpl-nprog3-dt2_6-z10\" # The name of the dataset to use\n",
    "config.data.num_files = 5  # Number of data files to use for this run (for quick tests)\n",
    "config.data.train_frac = 0.8  # 80% for training, 20% for validation\n",
    "\n",
    "# -- Model Architecture --\n",
    "config.model = config_dict.ConfigDict()\n",
    "config.model.d_in = 2  # Input feature dimension\n",
    "config.model.num_classes = 3  # Number of output classes for the progenitor classifier\n",
    "\n",
    "# Encoder (processes halo history)\n",
    "config.model.encoder = config_dict.ConfigDict({'name': 'gru', 'd_model': 128, 'd_out': 128, 'num_layers': 4})\n",
    "\n",
    "# Decoder (generates progenitor properties)\n",
    "config.model.decoder = config_dict.ConfigDict({'name': 'gru', 'd_model': 128, 'd_out': 128, 'num_layers': 4})\n",
    "\n",
    "# Neural Posterior Estimation (for sampling continuous properties)\n",
    "config.model.npe = config_dict.ConfigDict({'hidden_sizes': [128, 128], 'num_transforms': 4})\n",
    "\n",
    "# Classifier (predicts number of progenitors)\n",
    "config.model.classifier = config_dict.ConfigDict({'d_context': 1, 'hidden_sizes': [128, 128]})\n",
    "\n",
    "# -- Optimizer and Scheduler --\n",
    "config.optimizer = config_dict.ConfigDict({'name': 'AdamW', 'lr': 5e-5, 'weight_decay': 1e-4})\n",
    "config.scheduler = config_dict.ConfigDict({'name': 'WarmUpCosineAnnealingLR', 'warmup_steps': 5000, 'decay_steps': 100000})\n",
    "\n",
    "# -- Training Loop --\n",
    "config.training = config_dict.ConfigDict()\n",
    "config.training.max_epochs = 50\n",
    "config.training.max_steps = 100_000\n",
    "config.training.train_batch_size = 32  # Adjust based on your GPU memory\n",
    "config.training.eval_batch_size = 64\n",
    "config.training.num_workers = 4  # Number of CPU cores for data loading\n",
    "config.training.gradient_clip_val = 0.5\n",
    "config.training.monitor = 'val_loss' # Metric to monitor for checkpointing and early stopping\n",
    "config.training.patience = 10 # Early stopping patience in epochs\n",
    "config.training.save_top_k = 3 # Save the top 3 models\n",
    "\n",
    "print(f\"Configuration created for experiment: '{config.name}'\")\n",
    "print(f\"Training will run on: {config.accelerator}\")\n",
    "print(f\"Dataset to be used: {config.data.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c01f3c",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Next, we load the dataset using the parameters from our configuration object. The `prepare_dataloader` function handles reading the raw data, splitting it into training and validation sets, and creating `DataLoader` objects for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dad32ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "An unexpected error occurred: prepare_dataloader() got an unexpected keyword argument 'dataset_name'\n"
     ]
    }
   ],
   "source": [
    "# Set up the working directory for the experiment\n",
    "workdir = os.path.join(config.workdir, config.name)\n",
    "if os.path.exists(workdir) and config.overwrite:\n",
    "    print(f\"Overwriting existing directory: {workdir}\")\n",
    "    shutil.rmtree(workdir)\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "# Save the configuration to the experiment directory for reproducibility\n",
    "with open(os.path.join(workdir, 'config.yaml'), 'w') as f:\n",
    "    f.write(config.to_yaml())\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # This function handles loading, splitting, and creating DataLoaders\n",
    "    train_loader, val_loader, norm_dict = datasets.prepare_dataloader(\n",
    "        dataset_name=config.data.name,\n",
    "        dataset_root=config.data.root,\n",
    "        train_batch_size=config.training.train_batch_size,\n",
    "        eval_batch_size=config.training.eval_batch_size,\n",
    "        seed=config.seed.data,\n",
    "        num_data_files=config.data.num_files,\n",
    "        train_frac=config.data.train_frac,\n",
    "        num_workers=config.training.num_workers,\n",
    "    )\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"  -> Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"  -> Number of validation batches: {len(val_loader)}\")\n",
    "    print(f\"  -> Normalization dictionary loaded: {list(norm_dict.keys())}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {os.path.join(config.data.root, config.data.name)}\")\n",
    "    print(\"Please update the 'config.data.root' and 'config.data.name' paths in the configuration cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908fc85",
   "metadata": {},
   "source": [
    "## 4. Model Definition\n",
    "\n",
    "Now we instantiate the `AutoregTreeGen` model. This class, defined in `florah_tree/atg.py`, is a `pl.LightningModule` that encapsulates the entire model architecture, including the encoder, decoder, classifier, and the logic for training and validation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf279a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing the FLORAH model...\")\n",
    "\n",
    "# The model takes the configuration and the normalization dictionary (from the dataset) as input\n",
    "model = AutoregTreeGen(\n",
    "    d_in=config.model.d_in,\n",
    "    num_classes=config.model.num_classes,\n",
    "    encoder_args=config.model.encoder,\n",
    "    decoder_args=config.model.decoder,\n",
    "    npe_args=config.model.npe,\n",
    "    classifier_args=config.model.classifier,\n",
    "    optimizer_args=config.optimizer,\n",
    "    scheduler_args=config.scheduler,\n",
    "    training_args=config.training,\n",
    "    norm_dict=norm_dict,\n",
    ")\n",
    "\n",
    "# Print the number of parameters to get a sense of the model's size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model initialized successfully!\")\n",
    "print(f\" -> Total parameters: {total_params:,}\")\n",
    "print(f\" -> Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b4212",
   "metadata": {},
   "source": [
    "## 5. Training Setup\n",
    "\n",
    "With the data and model ready, we now configure the `Trainer`. This PyTorch Lightning object handles the training loop, gradient updates, evaluation, and logging. We also set up **callbacks** to add functionality like saving the best models (`ModelCheckpoint`) and stopping training if performance stagnates (`EarlyStopping`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12eec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks for the trainer\n",
    "callbacks = [\n",
    "    # Stop training if the validation loss doesn't improve for a number of epochs\n",
    "    pl.callbacks.EarlyStopping(\n",
    "        monitor=config.training.monitor,\n",
    "        patience=config.training.patience,\n",
    "        mode='min',\n",
    "        verbose=True\n",
    "    ),\n",
    "    # Save the best K models based on validation loss\n",
    "    pl.callbacks.ModelCheckpoint(\n",
    "        filename=\"best-model-{epoch}-{val_loss:.4f}\",\n",
    "        monitor=config.training.monitor,\n",
    "        save_top_k=config.training.save_top_k,\n",
    "        mode='min',\n",
    "    ),\n",
    "    # Save the last model checkpoint\n",
    "    pl.callbacks.ModelCheckpoint(filename=\"last-model-{epoch}\"),\n",
    "    # Monitor the learning rate\n",
    "    pl.callbacks.LearningRateMonitor(\"step\"),\n",
    "]\n",
    "\n",
    "# Set up the TensorBoard logger\n",
    "train_logger = pl_loggers.TensorBoardLogger(workdir, version='')\n",
    "\n",
    "# Initialize the PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    default_root_dir=workdir,\n",
    "    max_epochs=config.training.max_epochs,\n",
    "    max_steps=config.training.max_steps,\n",
    "    accelerator=config.accelerator,\n",
    "    callbacks=callbacks,\n",
    "    logger=train_logger,\n",
    "    gradient_clip_val=config.training.gradient_clip_val,\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured. Ready to start training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85a700",
   "metadata": {},
   "source": [
    "## 6. Execute Training\n",
    "\n",
    "This is the final step. Calling `trainer.fit()` will start the training process. The trainer will use the model, training data, and validation data we prepared earlier. Progress will be displayed below, and you can monitor more detailed metrics using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dd9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸš€ Starting training for '{config.name}'...\")\n",
    "\n",
    "# Set the seed for the training loop for reproducibility\n",
    "pl.seed_everything(config.seed.training)\n",
    "\n",
    "# Start the training!\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n",
    "print(\"\n",
    "ðŸŽ‰ Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ff78c",
   "metadata": {},
   "source": [
    "## 7. Review Results\n",
    "\n",
    "After training is complete, you can find the results in the directory specified in `config.workdir`. This includes the saved model checkpoints and TensorBoard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7411ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the best model checkpoint is stored in the callback\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "# You can list the contents of the experiment directory to see all saved files\n",
    "print(f\"\n",
    "Contents of the experiment directory ({workdir}):\")\n",
    "for item in os.listdir(workdir):\n",
    "    print(f\"- {item}\")\n",
    "\n",
    "# To view the training metrics, run TensorBoard in your terminal:\n",
    "print(\"\n",
    "To monitor training with TensorBoard, run this command in your terminal:\")\n",
    "print(f\"tensorboard --logdir={config.workdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2adeef",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations! You have successfully trained a FLORAH model.\n",
    "\n",
    "You can now use this trained model to generate new merger trees. Proceed to the **inference tutorial** (`tutorial_inference.ipynb`) to learn how to load your checkpoint and generate trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
