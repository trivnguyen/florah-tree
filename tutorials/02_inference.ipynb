{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc0f5c7",
   "metadata": {},
   "source": [
    "# FLORAH Tree: Inference Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44c543",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use a pre-trained `AutoregTreeGen` model to generate synthetic merger trees. We will cover the following steps:\n",
    "1. **Configuration**: Loading the necessary hyperparameters.\n",
    "2. **Load Model**: Loading a trained model from a checkpoint.\n",
    "3. **Prepare Input Data**: Setting up the initial conditions (root halos) for tree generation.\n",
    "4. **Generate Trees**: Running the inference process.\n",
    "5. **Save & Analyze Results**: Storing the generated trees and performing some basic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67424",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409c0f2a",
   "metadata": {},
   "source": [
    "First, we need to load the configuration file that was used for training the model. This ensures that the model architecture and other parameters are consistent. We will also define some inference-specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import ml_collections\n",
    "from ml_collections import config_flags\n",
    "from absl import flags\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import datasets\n",
    "from florah_tree import infer_utils, training_utils, models_utils, analysis_utils\n",
    "from florah_tree.atg import AutoregTreeGen\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load the same configuration file used for training\n",
    "config_path = '../configs/vsmdpl-nprog3-zmax10.py'\n",
    "\n",
    "# To avoid parsing flags in a notebook, we can manually load the config\n",
    "from ml_collections import config_dict\n",
    "import importlib.util\n",
    "\n",
    "spec = importlib.util.spec_from_file_location('config', config_path)\n",
    "config_module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(config_module)\n",
    "config = config_module.get_config()\n",
    "\n",
    "# --- Inference-specific settings ---\n",
    "# These settings are typically in `config.data_infer`\n",
    "config.data_infer = ml_collections.ConfigDict()\n",
    "config.data_infer.name = 'VMDPL'\n",
    "config.data_infer.root = '/path/to/your/simulation/data' # IMPORTANT: Update this path\n",
    "config.data_infer.box = 'VMDPL'\n",
    "config.data_infer.zmax = 10.0\n",
    "config.data_infer.step = 1\n",
    "config.data_infer.num_files = 1\n",
    "config.data_infer.num_max_trees = 100 # Number of trees to use as roots\n",
    "config.data_infer.multiplicative_factor = 1 # Generate this many trees per root\n",
    "config.data_infer.batch_size = 128\n",
    "config.data_infer.outdir = './inference_output'\n",
    "\n",
    "# --- Checkpoint settings ---\n",
    "# 'best', 'last', or a specific checkpoint file (e.g., 'epoch=1-step=100.ckpt')\n",
    "config.checkpoint_infer = 'best'\n",
    "\n",
    "print(\"Configuration loaded and updated for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b41dd",
   "metadata": {},
   "source": [
    "## 2. Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f1c8f",
   "metadata": {},
   "source": [
    "Now, we'll load the trained `AutoregTreeGen` model from the specified checkpoint. The script will automatically find the best or last checkpoint in the work directory if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614945ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if config.checkpoint_infer in ['best', 'last']:\n",
    "    checkpoint_dir = os.path.join(config.workdir, config.name, \"lightning_logs/checkpoints\")\n",
    "    all_checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\")))\n",
    "    if not all_checkpoints:\n",
    "        raise FileNotFoundError(f\"No checkpoints found in {checkpoint_dir}\")\n",
    "\n",
    "    if config.checkpoint_infer == 'best':\n",
    "        val_losses = []\n",
    "        for cp in all_checkpoints:\n",
    "            loss_match = re.search(r\"val_loss=([-+]?*\\.\\d+|\\d+)\", cp)\n",
    "            if loss_match:\n",
    "                val_losses.append(float(loss_match.group(1)))\n",
    "            else:\n",
    "                val_losses.append(float('inf')) # In case the name doesn't have the loss\n",
    "        checkpoint_path = all_checkpoints[np.argmin(val_losses)]\n",
    "    else: # 'last'\n",
    "        steps = []\n",
    "        for cp in all_checkpoints:\n",
    "            step_match = re.search(r\"step=(\\d+)\", cp)\n",
    "            if step_match:\n",
    "                steps.append(int(step_match.group(1)))\n",
    "            else:\n",
    "                steps.append(0)\n",
    "        checkpoint_path = all_checkpoints[np.argmax(steps)]\n",
    "else:\n",
    "    checkpoint_path = os.path.join(config.workdir, config.name, \"lightning_logs/checkpoints\", config.checkpoint_infer)\n",
    "\n",
    "print(f'Loading model from checkpoint: {checkpoint_path}')\n",
    "model = AutoregTreeGen.load_from_checkpoint(checkpoint_path, map_location=device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0262af",
   "metadata": {},
   "source": [
    "## 3. Prepare Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988a0a7",
   "metadata": {},
   "source": [
    "We need to provide the model with a set of root halos at redshift z=0. The model will then generate the merger history for these halos back in time. We also need to define the cosmic time (or redshift) steps at which the model will predict progenitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaf858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function reads snapshot times from metadata files.\n",
    "# You might need to adapt this or provide the times directly.\n",
    "DEFAULT_METADATA_DIR = \"../metadata\" # Assumes metadata is in project root\n",
    "def read_snapshot_times(box_name):\n",
    "    if \"GUREFT\" in box_name:\n",
    "        table_name = \"snapshot_times_gureft.txt\"\n",
    "    else:\n",
    "        table_name = f\"snapshot_times_{box_name.lower()}.txt\"\n",
    "    snapshot_times = np.genfromtxt(\n",
    "        os.path.join(DEFAULT_METADATA_DIR, table_name), delimiter=',', unpack=True)\n",
    "    return snapshot_times\n",
    "\n",
    "try:\n",
    "    # Get the root features from the simulation data\n",
    "    print(f\"Loading root halos from {config.data_infer.root}...\")\n",
    "    sim_data = datasets.read_dataset(\n",
    "        dataset_name=config.data_infer.name,\n",
    "        dataset_root=config.data_infer.root,\n",
    "        index_start=0, # Start from the first file\n",
    "        max_num_files=config.data_infer.num_files,\n",
    "    )\n",
    "    sim_data = sim_data[:config.data_infer.num_max_trees]\n",
    "    print(f\"  -> {len(sim_data)} root halos loaded.\")\n",
    "\n",
    "    # Get the time steps for inference\n",
    "    snap_table, aexp_table, z_table = read_snapshot_times(config.data_infer.box)\n",
    "    select = z_table <= config.data_infer.zmax\n",
    "    snap_times_out = snap_table[select][::config.data_infer.step]\n",
    "    aexp_times_out = aexp_table[select][::config.data_infer.step]\n",
    "\n",
    "    # Create the initial input tensor (x0) and the time history tensor (Zhist)\n",
    "    x0 = torch.stack([sim_data[i].x[0, :-1] for i in range(len(sim_data))], dim=0)\n",
    "    Zhist = torch.tensor(aexp_times_out, dtype=torch.float32).unsqueeze(1)\n",
    "    snapshot_list = torch.tensor(snap_times_out, dtype=torch.long)\n",
    "\n",
    "    # Repeat the input if you want to generate multiple trees from the same root\n",
    "    x0 = x0.repeat(config.data_infer.multiplicative_factor, 1)\n",
    "\n",
    "    print(f\"Input tensor shape (x0): {x0.shape}\")\n",
    "    print(f\"Time history tensor shape (Zhist): {Zhist.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Dataset not found at {os.path.join(config.data_infer.root)}\")\n",
    "    print(\"Please update the 'config.data_infer.root' path in the configuration cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37e23e",
   "metadata": {},
   "source": [
    "## 4. Generate Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae38f8c6",
   "metadata": {},
   "source": [
    "With the model loaded and the inputs prepared, we can now run the autoregressive generation process. The `generate_forest` utility function handles the batching and the step-by-step generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4dca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "pl.seed_everything(42)\n",
    "\n",
    "print(\"Starting tree generation...\")\n",
    "tree_list = infer_utils.generate_forest(\n",
    "    model,\n",
    "    x0,\n",
    "    Zhist,\n",
    "    norm_dict=model.norm_dict,\n",
    "    device=device,\n",
    "    batch_size=config.data_infer.batch_size,\n",
    "    sort=True,\n",
    "    snapshot_list=snapshot_list,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"Finished generation. {len(tree_list)} trees were created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e8935",
   "metadata": {},
   "source": [
    "## 5. Save & Analyze Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e806022",
   "metadata": {},
   "source": [
    "Finally, we'll save the list of generated trees to a file. The output is a list of `torch_geometric.data.Data` objects, where each object represents a single merger tree. We can use `pickle` to save this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a650cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(config.data_infer.outdir, exist_ok=True)\n",
    "outfile = os.path.join(config.data_infer.outdir, 'generated_trees.pkl')\n",
    "\n",
    "print(f\"Saving generated trees to {outfile}\")\n",
    "with open(outfile, 'wb') as f:\n",
    "    pickle.dump(tree_list, f)\n",
    "print(\"Save complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd6587",
   "metadata": {},
   "source": [
    "### Basic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c57fc",
   "metadata": {},
   "source": [
    "Let's load the saved trees and inspect one of them to see what the output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trees back from the file\n",
    "with open(outfile, 'rb') as f:\n",
    "    loaded_trees = pickle.load(f)\n",
    "\n",
    "# Inspect the first tree\n",
    "if loaded_trees:\n",
    "    first_tree = loaded_trees[0]\n",
    "    print(\"--- First Generated Tree ---\")\n",
    "    print(first_tree)\n",
    "    print(f\"Number of nodes (halos): {first_tree.num_nodes}\")\n",
    "    print(f\"Number of edges (progenitor links): {first_tree.num_edges}\")\n",
    "    print(f\"Node features shape: {first_tree.x.shape}\")\n",
    "    print(f\"Node features (first 5 nodes): {first_tree.x[:5]}\")\n",
    "else:\n",
    "    print(\"No trees were generated or loaded.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
