{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/projects/florah/florah-tree\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/home/tnguyen/projects/florah/florah-tree\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from ml_collections import config_dict\n",
    "\n",
    "from florah_analysis import tree_utils, utils\n",
    "\n",
    "from models import training_utils, models, models_utils, flows_utils\n",
    "from models import generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(\n",
    "    data, train_frac=0.8, batch_size=1024, num_workers=1,\n",
    "    norm_dict=None):\n",
    "\n",
    "    num_total = len(data)\n",
    "    num_train = int(num_total * train_frac)\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # calculate the normaliziation statistics\n",
    "    x = torch.cat([d.x for d in data[:num_train]])\n",
    "    if norm_dict is None:\n",
    "        x_loc = x.mean(dim=0)\n",
    "        x_scale = x.std(dim=0)\n",
    "        norm_dict = {\n",
    "            \"x_loc\": list(x_loc.numpy()),\n",
    "            \"x_scale\": list(x_scale.numpy()),\n",
    "        }\n",
    "    else:\n",
    "        x_loc = torch.tensor(norm_dict[\"x_loc\"], dtype=torch.float)\n",
    "        x_scale = torch.tensor(norm_dict[\"x_scale\"], dtype=torch.float)\n",
    "    for d in data:\n",
    "        d.x = (d.x - x_loc) / x_scale\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data[:num_train], batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(\n",
    "        data[num_train:], batch_size=batch_size, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/torch_geometric/utils/convert.py:249: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "# Fake config \n",
    "config = config_dict.ConfigDict()\n",
    "config.workdir = './logging/'\n",
    "config.name = 'GUREFT05-Nanc1.transfGenerator'\n",
    "\n",
    "# Read in the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint_path = \"/mnt/home/tnguyen/projects/florah/florah-tree/\"\\\n",
    "    \"logging/GUREFT05-Nanc1.transfGenerator/version_0/checkpoints/epoch=137-step=43056.ckpt\"\n",
    "model = generator.TreeGenerator.load_from_checkpoint(\n",
    "    checkpoint_path, map_location=device)\n",
    "norm_dict = model.norm_dict\n",
    "\n",
    "# Read in the dataset\n",
    "dset_path = '/mnt/ceph/users/tnguyen/florah/datasets/experiments/GUREFT05-Nanc1.debug.pkl'\n",
    "with open(dset_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# convert networkx to pytorch geometric\n",
    "data_pyg = [from_networkx(d) for d in data]\n",
    "train_loader, val_loader, norm_dict = prepare_dataloader(\n",
    "    data_pyg, batch_size=1, norm_dict=norm_dict)\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tree(\n",
    "    model, root_halo_feat, t_out, n_max_iter=1000, norm_dict=None):\n",
    "    \"\"\"Generate a tree from a root halo feature vector. \n",
    "    Assume root_halo_feat and t_out are already normalized.\n",
    "    \"\"\"\n",
    "    # create list of output times\n",
    "    if not isinstance(t_out, torch.Tensor):\n",
    "        t_out = torch.tensor(t_out, dtype=torch.float32)\n",
    "    t_out = t_out.to(device)\n",
    "\n",
    "    if not isinstance(root_halo_feat, torch.Tensor):\n",
    "        root_halo_feat = torch.tensor(root_halo_feat, dtype=torch.float32)\n",
    "    root_halo_feat = root_halo_feat.to(device)\n",
    "    root_halo_feat = torch.cat([root_halo_feat, t_out[0].unsqueeze(0)])\n",
    "\n",
    "    # initialize the tree\n",
    "    halo_t_index = [0]\n",
    "    halo_index = [0]\n",
    "    halo_remain_index = [0]  # index of the remining halos to be processed\n",
    "    next_halo_index = 1\n",
    "    halo_feats = torch.stack([root_halo_feat], dim=0)\n",
    "    edge_index= [[], []]  # keep edge index as a list to make appending easier\n",
    "    next_halo_index = 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        while (len(halo_remain_index) > 0) & (n_max_iter > 0):\n",
    "            halo_curr_index = halo_remain_index.pop(0)\n",
    "            halo_curr_t_index = halo_t_index.pop(0)\n",
    "            if halo_curr_t_index == len(t_out) - 1:\n",
    "                # reach the last snapshot\n",
    "                continue\n",
    "            t_next = t_out[halo_curr_t_index + 1]\n",
    "            t_next = t_next.unsqueeze(0).unsqueeze(0)  # add batch and feature dim\n",
    "\n",
    "            # input features\n",
    "            path = training_utils.find_path_from_root(\n",
    "                torch.tensor(edge_index, dtype=torch.long),\n",
    "                halo_curr_index)\n",
    "            x_feat = model.featurizer(halo_feats[path].unsqueeze(0)) # add batch dim\n",
    "            t_proj = model.time_proj_layer(t_next)\n",
    "\n",
    "            # randomly sample the number of progenitors\n",
    "            x_classifier = torch.cat((x_feat, t_proj), dim=1)\n",
    "            yhat = model.classifier(x_classifier).softmax(dim=1)\n",
    "            num_progs = torch.multinomial(yhat, 1) + 1\n",
    "            num_progs = num_progs.item()\n",
    "\n",
    "            # list of progenitors, including the zeros starting token\n",
    "            x_progs = torch.zeros((1, num_progs + 1, 2), device=device)\n",
    "            t_proj_progs = t_proj.unsqueeze(1).repeat(1, num_progs + 1, 1)\n",
    "\n",
    "            for i_prog in range(num_progs):\n",
    "                n_prog_curr = i_prog + 1  # number of progenitors at current step\n",
    "\n",
    "                lengths = torch.tensor([n_prog_curr], dtype=torch.long)\n",
    "                f_proj = model.feat_proj_layer(x_progs)\n",
    "\n",
    "                # run the RNN to extract the context\n",
    "                x_rnn = torch.cat([f_proj, t_proj_progs], dim=-1)\n",
    "                x_rnn = model.rnn(x_rnn, lengths=lengths)\n",
    "\n",
    "                # # sample from the flow\n",
    "                flow_context = torch.cat(\n",
    "                    [x_rnn, x_feat.unsqueeze(1).repeat(1, n_prog_curr, 1)], dim=-1)\n",
    "                flow_context = flow_context[:, -1]  # only take the last time step\n",
    "                x_prog_curr = model.flows.sample(1, context=flow_context)\n",
    "\n",
    "                # append to the list of progenitors\n",
    "                x_progs[:, i_prog + 1] = x_prog_curr\n",
    "            x_progs = x_progs[:, 1:]  # remove the zeros starting token]\n",
    "\n",
    "            # add the progenitors to the list of halos\n",
    "            x_progs = torch.cat([x_progs, t_next.repeat(1, num_progs, 1)], dim=-1)\n",
    "            x_progs = x_progs.squeeze(0)  # remove the batch dim\n",
    "            halo_feats = torch.cat([halo_feats, x_progs], dim=0)\n",
    "\n",
    "            # create halo index for the progneitors\n",
    "            prog_index = [next_halo_index + i for i in range(num_progs)]\n",
    "            halo_index = halo_index + prog_index\n",
    "            halo_remain_index = halo_remain_index + prog_index\n",
    "            halo_t_index = halo_t_index + [halo_curr_t_index + 1] * num_progs\n",
    "\n",
    "            # create edge index for the progenitors\n",
    "            edge_index[0] = edge_index[0] + [halo_curr_index] * num_progs\n",
    "            edge_index[1] = edge_index[1] + prog_index\n",
    "            next_halo_index += num_progs\n",
    "\n",
    "            n_max_iter -= 1\n",
    "            if n_max_iter == 0:\n",
    "                print('Max number of iterations reached')\n",
    "\n",
    "    data = Data(x=halo_feats, edge_index=torch.tensor(edge_index, dtype=torch.long))\n",
    "\n",
    "    if norm_dict is not None:\n",
    "        x_loc_norm = torch.tensor(\n",
    "            norm_dict['x_loc'], dtype=torch.float32, device=device)\n",
    "        x_scale_norm = torch.tensor(\n",
    "            norm_dict['x_scale'], dtype=torch.float32, device=device)\n",
    "        data.x = data.x * x_scale_norm + x_loc_norm\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "snaps, aexp_snaps, z_snaps = utils.read_snapshot_times('GUREFT05')\n",
    "num_snap_max = np.where(z_snaps > 10)[0][0]\n",
    "step = 2\n",
    "aexp_out = aexp_snaps[:num_snap_max:step]\n",
    "x_loc_norm = torch.tensor(norm_dict['x_loc'], dtype=torch.float32)\n",
    "x_scale_norm = torch.tensor(norm_dict['x_scale'], dtype=torch.float32)\n",
    "\n",
    "# create list of output times\n",
    "t_out = torch.tensor(aexp_out, dtype=torch.float32)\n",
    "t_out = (t_out - x_loc_norm[-1]) / x_scale_norm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/networkx/drawing/nx_pylab.py:304: UserWarning: \n",
      "\n",
      "The arrowsize keyword argument is not applicable when drawing edges\n",
      "with LineCollection.\n",
      "\n",
      "To make this warning go away, either specify `arrows=True` to\n",
      "force FancyArrowPatches or use the default value for arrowsize.\n",
      "Note that using FancyArrowPatches may be slow for large graphs.\n",
      "\n",
      "  draw_networkx_edges(G, pos, arrows=arrows, **edge_kwds)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 800x800 with 1 Axes>,\n",
       " <Axes: title={'center': 'Merger Tree Graph'}>)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get an initial mass and concentration\n",
    "root_halo_feat = batch.x[8][:-1]\n",
    "data = generate_tree(model, root_halo_feat, t_out, norm_dict=norm_dict)\n",
    "data = data.cpu()\n",
    "\n",
    "# convert to a networkx graph manually since the to_networkx method is not working\n",
    "# for some reason\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(data.x)))\n",
    "G.add_edges_from(data.edge_index.T.numpy())\n",
    "for i in range(len(data.x)):\n",
    "    G.nodes[i]['x'] = data.x[i].numpy()\n",
    "\n",
    "tree_utils.plot_graph(G)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
