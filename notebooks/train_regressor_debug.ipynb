{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/projects/florah/florah-tree\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%cd /mnt/home/tnguyen/projects/florah/florah-tree\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import torch_geometric\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from ml_collections import config_dict\n",
    "\n",
    "from models import training_utils, models, models_utils, flows_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/tnguyen/miniconda3/envs/geometric/lib/python3.11/site-packages/torch_geometric/utils/convert.py:249: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "dset_path = '/mnt/ceph/users/tnguyen/florah/datasets/experiments/GUREFT05-Nanc1.debug.pkl'\n",
    "with open(dset_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# convert networkx to pytorch geometric\n",
    "data = [from_networkx(d) for d in data]\n",
    "\n",
    "def prepare_dataloader(data, train_frac=0.8, batch_size=1024, num_workers=1):\n",
    "\n",
    "    num_total = len(data)\n",
    "    num_train = int(num_total * train_frac)\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # calculate the normaliziation statistics\n",
    "    x = torch.cat([d.x for d in data[:num_train]])\n",
    "    x_mean = x.mean(dim=0)\n",
    "    x_std = x.std(dim=0)\n",
    "    norm_dict = {\n",
    "        \"x_mean\": list(x_mean.numpy()),\n",
    "        \"x_std\": list(x_std.numpy()),\n",
    "    }\n",
    "    for d in data:\n",
    "        d.x = (d.x - x_mean) / x_std\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        data[:num_train], batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "    val_loader = DataLoader(\n",
    "        data[num_train:], batch_size=batch_size, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, norm_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake config for debugging\n",
    "config = config_dict.ConfigDict()\n",
    "config.input_size = 3\n",
    "config.d_time = 1\n",
    "config.d_time_projection = 64\n",
    "config.d_feat_projection = 64\n",
    "config.sum_features = False\n",
    "\n",
    "# featurizer parameters\n",
    "config.featurizer = config_dict.ConfigDict()\n",
    "config.featurizer.d_model = 64\n",
    "config.featurizer.nhead = 4\n",
    "config.featurizer.num_encoder_layers = 2\n",
    "config.featurizer.dim_feedforward = 128\n",
    "config.featurizer.use_embedding = True\n",
    "\n",
    "# rnn parameters\n",
    "config.rnn = config_dict.ConfigDict()\n",
    "config.rnn.output_size = 64\n",
    "config.rnn.hidden_size = 64\n",
    "config.rnn.num_layers = 2\n",
    "\n",
    "# flows parameters\n",
    "config.flows = config_dict.ConfigDict()\n",
    "config.flows.hidden_size = 64\n",
    "config.flows.num_blocks = 2\n",
    "config.flows.num_layers = 2\n",
    "\n",
    "featurizer = models.TransformerFeaturizer(\n",
    "    input_size=config.input_size,\n",
    "    d_model=config.featurizer.d_model,\n",
    "    nhead=config.featurizer.nhead,\n",
    "    num_encoder_layers=config.featurizer.num_encoder_layers,\n",
    "    dim_feedforward=config.featurizer.dim_feedforward,\n",
    "    use_embedding=config.featurizer.use_embedding,\n",
    ")\n",
    "rnn = models.GRUModel(\n",
    "    input_size=config.d_feat_projection + config.d_time_projection,\n",
    "    output_size=config.rnn.output_size,\n",
    "    hidden_size=config.rnn.hidden_size,\n",
    "    num_layers=config.rnn.num_layers,\n",
    ")\n",
    "flows = flows_utils.build_maf(\n",
    "    features=config.input_size - config.d_time,\n",
    "    hidden_features=config.flows.hidden_size,\n",
    "    context_features=config.rnn.output_size + config.featurizer.d_model,\n",
    "    num_layers=config.flows.num_layers,\n",
    "    num_blocks=config.flows.num_blocks,\n",
    ")\n",
    "\n",
    "time_projection_layer = nn.Linear(\n",
    "    config.d_time, config.d_time_projection)\n",
    "feat_projection_layer = nn.Linear(\n",
    "    config.input_size - config.d_time, config.d_feat_projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, norm_dict = prepare_dataloader(data, batch_size=32)\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "batch  = training_utils.prepare_batch(batch, num_samples_per_graph=1)\n",
    "padded_features = batch[0]\n",
    "lengths = batch[1]\n",
    "padded_out_features = batch[2]\n",
    "out_lengths = batch[3]\n",
    "\n",
    "# Separate the time and feature dimensions of the output\n",
    "t_out = padded_out_features[:, 0, -config.d_time:]\n",
    "f_out = padded_out_features[:, :, :-config.d_time]\n",
    "\n",
    "# add a starting token of all zeros to the first time step of padded_out_features\n",
    "# this is the input to the RNN\n",
    "padded_rnn_features = nn.functional.pad(\n",
    "    f_out, (0, 0, 1, 0), value=0)\n",
    "\n",
    "# divide the rnn into the input and output component\n",
    "# the input will be feed into the RNN, while the output will be used for the flow loss\n",
    "padded_rnn_input = padded_rnn_features[:, :-1, :]\n",
    "padded_rnn_output = padded_rnn_features[:, 1:, :]\n",
    "\n",
    "# Assuming padded_features is your input to the transformer\n",
    "# with shape [batch_size, seq_len, feature_size]\n",
    "batch_size, seq_len, _ = padded_features.size()\n",
    "out_seq_len = padded_out_features.size(1)\n",
    "\n",
    "# Create a mask for padding (assuming padding tokens are zero)\n",
    "# The mask should have the shape [seq_len, batch_size]\n",
    "transformer_padding_mask = training_utils.create_padding_mask(\n",
    "    lengths, seq_len, batch_first=True)\n",
    "rnn_padding_mask = training_utils.create_padding_mask(\n",
    "    out_lengths, out_seq_len, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = featurizer(padded_features, src_key_padding_mask=transformer_padding_mask)\n",
    "x = x.masked_fill(transformer_padding_mask.unsqueeze(-1), 0)\n",
    "x2 = x.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.eval()\n",
    "x = featurizer(padded_features, src_key_padding_mask=transformer_padding_mask)\n",
    "\n",
    "if not config.sum_features:\n",
    "    lengths = transformer_padding_mask.eq(0).sum(dim=1)\n",
    "    x = x[torch.arange(batch_size).to(x.device), lengths-1]\n",
    "else:\n",
    "    # set all the padding tokens to zero\n",
    "    x = x.masked_fill(transformer_padding_mask.unsqueeze(-1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0466,  1.6705, -0.3154,  ...,  0.1360, -0.3569,  0.6613],\n",
       "        [ 1.0263,  1.7973, -0.5968,  ..., -0.0718, -0.2929,  0.8572],\n",
       "        [ 0.9830,  1.8787, -0.8677,  ..., -0.2584, -0.2375,  1.0461],\n",
       "        ...,\n",
       "        [ 1.0903,  1.9821, -0.5649,  ..., -1.1361,  0.5769,  0.2221],\n",
       "        [ 1.0754,  1.9864, -0.5998,  ..., -1.2472,  0.6409,  0.2055],\n",
       "        [ 1.0594,  1.9078, -0.2549,  ..., -1.3134,  0.7642, -0.0770]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   summed_features = torch.stack([torch.sum(seq[:length], dim=0) \n",
    "                                   for seq, length in zip(x, original_lengths)])\n",
    "\n",
    "    # Select the last features for each sequence in the batch\n",
    "    last_features = torch.stack([seq[length - 1] \n",
    "                                 for seq, length in zip(x, original_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        x = x.masked_select(\n",
    "            transformer_padding_mask.unsqueeze(-1).repeat(1, 1, x.size(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 13, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the feature extractor\n",
    "x = x.sum(dim=1) if config.sum_features else x[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# project the time and feature dimensions of the output\n",
    "t_proj = time_projection_layer(t_out)\n",
    "f_proj = feat_projection_layer(padded_rnn_input)\n",
    "\n",
    "# create the input for the RNN and run\n",
    "x_rnn = torch.cat(\n",
    "    [f_proj, t_proj.unsqueeze(1).repeat(1, out_seq_len, 1)], dim=-1)\n",
    "x_rnn, hout = rnn(x_rnn, out_lengths, return_hidden_states=True)\n",
    "\n",
    "# create the context and input for the flows\n",
    "flow_context = torch.cat(\n",
    "    [x_rnn, x.unsqueeze(1).repeat(1, out_seq_len, 1)], dim=-1)\n",
    "flow_context = flow_context[~rnn_padding_mask]\n",
    "x_flow = f_out[~rnn_padding_mask]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceRegressor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning module that combines a Transformer-based feature extractor,\n",
    "    an RNN, and a normalizing flow to regress a sequence of features.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    input_size : int\n",
    "        The size of the input\n",
    "    d_feat_projection : int\n",
    "        The dimension of the feature projection layer.\n",
    "    d_time : int\n",
    "        The dimension of the time projection layer.\n",
    "    d_time_projection : int\n",
    "        The dimension of the time projection layer.\n",
    "    sum_features : bool\n",
    "        Whether to sum the features of the featurizer in the time dimension.\n",
    "    num_samples_per_graph : int\n",
    "        The number of samples per graph.\n",
    "    batch_first : bool\n",
    "        Whether the input is batch first.\n",
    "    featurizer : nn.Module\n",
    "        The featurizer.\n",
    "    rnn : nn.Module\n",
    "        The RNN.\n",
    "    flows : nn.Module\n",
    "        The normalizing flow.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer.\n",
    "    scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The scheduler.\n",
    "    norm_dict : dict\n",
    "        The normalization dictionary. For bookkeeping purposes only.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        featurizer_args,\n",
    "        rnn_args,\n",
    "        flows_args,\n",
    "        optimizer_args=None,\n",
    "        scheduler_args=None,\n",
    "        sum_features=False,\n",
    "        d_time = 1,\n",
    "        d_time_projection = 128,\n",
    "        d_feat_projection = 128,\n",
    "        num_samples_per_graph=1,\n",
    "        norm_dict=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            The size of the input\n",
    "        num_classes : int\n",
    "            The number of classes\n",
    "        featurizer_args : dict\n",
    "            Arguments for the featurizer\n",
    "        rnn_args : dict\n",
    "            Arguments for the RNN\n",
    "        flows_args : dict\n",
    "            Arguments for the normalizing flow\n",
    "        optimizer_args : dict, optional\n",
    "            Arguments for the optimizer. Default: None\n",
    "        scheduler_args : dict, optional\n",
    "            Arguments for the scheduler. Default: None\n",
    "        sum_features : bool, optional\n",
    "            Whether to sum the features of the featurizer in the time dimension.\n",
    "            Default: False\n",
    "        d_time : int, optional\n",
    "            The dimension of the time projection layer. Default: 1\n",
    "        d_time_projection : int, optional\n",
    "            The dimension of the time projection layer. Default: 1\n",
    "        d_feat_projection : int, optional\n",
    "            The dimension of the feature projection layer. Default: 1\n",
    "        num_samples_per_graph : int, optional\n",
    "            The number of samples per graph. Default: 1\n",
    "        norm_dict : dict, optional\n",
    "            The normalization dictionary. For bookkeeping purposes only.\n",
    "            Default: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.featurizer_args = featurizer_args\n",
    "        self.rnn_args = rnn_args\n",
    "        self.flows_args = flows_args\n",
    "        self.optimizer_args = optimizer_args or {}\n",
    "        self.scheduler_args = scheduler_args or {}\n",
    "        self.sum_features = sum_features\n",
    "        self.d_time = d_time\n",
    "        self.d_time_projection = d_time_projection\n",
    "        self.d_feat_projection = d_feat_projection\n",
    "        self.num_samples_per_graph = num_samples_per_graph\n",
    "        self.norm_dict = norm_dict\n",
    "        self.batch_first = True # always True\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self._setup_model()\n",
    "\n",
    "    def _setup_model(self):\n",
    "\n",
    "        # create the featurizer\n",
    "        if self.featurizer_args.name == 'transformer':\n",
    "            activation_fn = models_utils.get_activation(\n",
    "                self.featurizer_args.activation)\n",
    "            self.featurizer = models.TransformerFeaturizer(\n",
    "                input_size=self.input_size,\n",
    "                d_model=self.featurizer_args.d_model,\n",
    "                nhead=self.featurizer_args.nhead,\n",
    "                num_encoder_layers=self.featurizer_args.num_encoder_layers,\n",
    "                dim_feedforward=self.featurizer_args.dim_feedforward,\n",
    "                batch_first=self.batch_first,\n",
    "                use_embedding=self.featurizer_args.use_embedding,\n",
    "                activation_fn=activation_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'Featurizer {featurizer_name} not supported')\n",
    "\n",
    "        # create the rnn\n",
    "        if self.rnn_args.name == 'gru':\n",
    "            activation_fn = models_utils.get_activation(\n",
    "                self.rnn_args.activation)\n",
    "            self.rnn = models.GRUModel(\n",
    "                input_size=config.d_feat_projection + config.d_time_projection,\n",
    "                output_size=self.rnn_args.output_size,\n",
    "                hidden_size=self.rnn_args.hidden_size,\n",
    "                num_layers=self.rnn_args.num_layers,\n",
    "                activation_fn=activation_fn,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'RNN {rnn_name} not supported')\n",
    "\n",
    "        # create the flows\n",
    "        self.flows = flows_utils.build_maf(\n",
    "            features=self.input_size - self.d_time,\n",
    "            hidden_features=self.flows_args.hidden_size,\n",
    "            context_features=self.rnn_args.output_size + self.featurizer_args.d_model,\n",
    "            num_layers=self.flows_args.num_layers,\n",
    "            num_blocks=self.flows_args.num_blocks,\n",
    "        )\n",
    "\n",
    "        # create the projection layers\n",
    "        self.time_proj_layer = nn.Linear(self.d_time, self.d_time_projection)\n",
    "        self.feat_proj_layer = nn.Linear(\n",
    "            self.input_size - self.d_time, self.d_feat_projection)\n",
    "        \n",
    "    def _prepare_batch(self, batch):\n",
    "        \"\"\" Prepare the batch for training. \"\"\"\n",
    "        batch  = training_utils.prepare_batch(batch, num_samples_per_graph=1)\n",
    "        padded_features = batch[0]\n",
    "        lengths = batch[1]\n",
    "        padded_out_features = batch[2]\n",
    "        out_lengths = batch[3]\n",
    "\n",
    "        # Separate the time and feature dimensions of the output\n",
    "        t_out = padded_out_features[:, 0, -self.d_time:]\n",
    "        f_out = padded_out_features[:, :, :-self.d_time]\n",
    "\n",
    "        # add a starting token of all zeros to the first time step of padded_out_features\n",
    "        # this is the input to the RNN\n",
    "        padded_rnn_features = nn.functional.pad(f_out, (0, 0, 1, 0), value=0)\n",
    "\n",
    "        # divide the rnn into the input and output component\n",
    "        # the input will be feed into the RNN, \n",
    "        # while the output will be used for the flow loss\n",
    "        padded_rnn_input = padded_rnn_features[:, :-1, :]\n",
    "        padded_rnn_output = padded_rnn_features[:, 1:, :]\n",
    "\n",
    "        # Assuming padded_features is your input to the transformer\n",
    "        # with shape [batch_size, seq_len, feature_size]\n",
    "        batch_size, seq_len, _ = padded_features.size()\n",
    "        out_seq_len = padded_out_features.size(1)\n",
    "\n",
    "        # Create a mask for padding (assuming padding tokens are zero)\n",
    "        # The mask should have the shape [seq_len, batch_size]\n",
    "        transformer_padding_mask = training_utils.create_padding_mask(\n",
    "            lengths, seq_len, batch_first=True)\n",
    "        rnn_padding_mask = training_utils.create_padding_mask(\n",
    "            out_lengths, out_seq_len, batch_first=True)\n",
    "\n",
    "        # Move to the same device as the model\n",
    "        padded_features = padded_features.to(self.device)\n",
    "        padded_rnn_input = padded_rnn_input.to(self.device)\n",
    "        padded_rnn_output = padded_rnn_output.to(self.device)\n",
    "        transformer_padding_mask = transformer_padding_mask.to(self.device)\n",
    "        rnn_padding_mask = rnn_padding_mask.to(self.device)\n",
    "        t_out = t_out.to(self.device)\n",
    "\n",
    "        # return a dictionary of the inputs\n",
    "        return_dict = {\n",
    "            'padded_features': padded_features,\n",
    "            'padded_rnn_input': padded_rnn_input,\n",
    "            'padded_rnn_output': padded_rnn_output,\n",
    "            'transformer_padding_mask': transformer_padding_mask,\n",
    "            'rnn_padding_mask': rnn_padding_mask,\n",
    "            't_out': t_out,\n",
    "            'batch_size': batch_size,\n",
    "            'seq_len': seq_len,\n",
    "            'out_seq_len': out_seq_len,\n",
    "        }\n",
    "        return return_dict\n",
    "        \n",
    "    def forward(\n",
    "        self, padded_features, padded_rnn_input, t_out,\n",
    "        transformer_padding_mask=None, rnn_padding_mask=None\n",
    "    ):\n",
    "        # extract the features\n",
    "        x = self.featurizer(\n",
    "            padded_features, src_key_padding_mask=transformer_padding_mask)\n",
    "        x = x.sum(dim=1) if self.sum_features else x[:, -1]\n",
    "\n",
    "        # project the time and feature dimensions\n",
    "        t_proj = self.time_proj_layer(t_out)\n",
    "        f_proj = self.feat_proj_layer(padded_rnn_input)\n",
    "\n",
    "        # create the input for the RNN \n",
    "        out_seq_len = padded_rnn_input.size(1)  # lengths after padding\n",
    "        out_lengths = rnn_padding_mask.eq(0).sum(-1) # original lengths\n",
    "        x_rnn = torch.cat(\n",
    "            [f_proj, t_proj.unsqueeze(1).repeat(1, out_seq_len, 1)], dim=-1)\n",
    "        x_rnn = self.rnn(x_rnn, out_lengths)\n",
    "\n",
    "        # create the context and input for the flows\n",
    "        flow_context = torch.cat(\n",
    "            [x_rnn, x.unsqueeze(1).repeat(1, out_seq_len, 1)], dim=-1)\n",
    "        flow_context = flow_context[~rnn_padding_mask]\n",
    "\n",
    "        return flow_context\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        batch_dict = self._prepare_batch(batch)\n",
    "\n",
    "        flow_context = self.forward(\n",
    "            padded_features=batch_dict['padded_features'],\n",
    "            padded_rnn_input=batch_dict['padded_rnn_input'],\n",
    "            t_out=batch_dict['t_out'],\n",
    "            transformer_padding_mask=batch_dict['transformer_padding_mask'],\n",
    "            rnn_padding_mask=batch_dict['rnn_padding_mask'],\n",
    "        )\n",
    "        x_flow = batch_dict['padded_rnn_output'][~batch_dict['rnn_padding_mask']]\n",
    "        log_prob = self.flows.log_prob(x_flow, context=flow_context)\n",
    "        loss = -log_prob.mean()\n",
    "\n",
    "        # log the loss\n",
    "        self.log(\n",
    "            'train_loss', loss, on_step=True, on_epoch=True, logger=True,\n",
    "            prog_bar=True, batch_size=batch_dict['batch_size'])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        batch_dict = self._prepare_batch(batch)\n",
    "\n",
    "        flow_context = self.forward(\n",
    "            padded_features=batch_dict['padded_features'],\n",
    "            padded_rnn_input=batch_dict['padded_rnn_input'],\n",
    "            t_out=batch_dict['t_out'],\n",
    "            transformer_padding_mask=batch_dict['transformer_padding_mask'],\n",
    "            rnn_padding_mask=batch_dict['rnn_padding_mask'],\n",
    "        )\n",
    "        x_flow = batch_dict['padded_rnn_output'][~batch_dict['rnn_padding_mask']]\n",
    "        log_prob = self.flows.log_prob(x_flow, context=flow_context)\n",
    "        loss = -log_prob.mean()\n",
    "\n",
    "        # log the loss\n",
    "        self.log(\n",
    "            'val_loss', loss, on_step=True, on_epoch=True, logger=True,\n",
    "            prog_bar=True, batch_size=batch_dict['batch_size'])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Initialize optimizer and LR scheduler \"\"\"\n",
    "\n",
    "        # setup the optimizer\n",
    "        if self.optimizer_args.name == \"Adam\":\n",
    "            return torch.optim.Adam(\n",
    "                self.parameters(), lr=self.optimizer_args.lr,\n",
    "                weight_decay=self.optimizer_args.weight_decay)\n",
    "        elif self.optimizer_args.name == \"AdamW\":\n",
    "            return torch.optim.AdamW(\n",
    "                self.parameters(), lr=self.optimizer_args.lr,\n",
    "                weight_decay=self.optimizer_args.weight_decay)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Optimizer {} not implemented\".format(self.optimizer_args.name))\n",
    "\n",
    "        # setup the scheduler\n",
    "        if self.scheduler_args.get(name) is None:\n",
    "            scheduler = None\n",
    "        elif self.scheduler_args.name == 'ReduceLROnPlateau':\n",
    "            scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, 'min', factor=self.scheduler_args.factor,\n",
    "                patience=self.scheduler_args.patience)\n",
    "        elif self.scheduler_args.name == 'CosineAnnealingLR':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=self.scheduler_args.T_max,)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Scheduler {} not implemented\".format(self.scheduler_args.name))\n",
    "\n",
    "        if scheduler is None:\n",
    "            return optimizer\n",
    "        else:\n",
    "            return {\n",
    "                'optimizer': optimizer,\n",
    "                'lr_scheduler': {\n",
    "                    'scheduler': scheduler,\n",
    "                    'monitor': 'train_loss',\n",
    "                    'interval': 'epoch',\n",
    "                    'frequency': 1\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake config for debugging\n",
    "config = config_dict.ConfigDict()\n",
    "config.input_size = 3\n",
    "config.d_time = 1\n",
    "config.d_time_projection = 64\n",
    "config.d_feat_projection = 64\n",
    "config.sum_features = False\n",
    "\n",
    "# featurizer parameters\n",
    "config.featurizer = config_dict.ConfigDict()\n",
    "config.featurizer.name = 'transformer'\n",
    "config.featurizer.d_model = 64\n",
    "config.featurizer.nhead = 4\n",
    "config.featurizer.num_encoder_layers = 2\n",
    "config.featurizer.dim_feedforward = 128\n",
    "config.featurizer.use_embedding = True\n",
    "config.featurizer.activation = config_dict.ConfigDict()\n",
    "config.featurizer.activation.name = 'identity'\n",
    "\n",
    "# rnn parameters\n",
    "config.rnn = config_dict.ConfigDict()\n",
    "config.rnn.name = 'gru'\n",
    "config.rnn.output_size = 64\n",
    "config.rnn.hidden_size = 64\n",
    "config.rnn.num_layers = 2\n",
    "config.rnn.activation = config_dict.ConfigDict()\n",
    "config.rnn.activation.name = 'relu'\n",
    "\n",
    "# flows parameters\n",
    "config.flows = config_dict.ConfigDict()\n",
    "config.flows.name = 'maf'\n",
    "config.flows.hidden_size = 64\n",
    "config.flows.num_blocks = 2\n",
    "config.flows.num_layers = 2\n",
    "config.flows.activation = config_dict.ConfigDict()\n",
    "config.flows.activation.name = 'tanh'\n",
    "\n",
    "# optimizer and scheduler configuration\n",
    "config.optimizer = config_dict.ConfigDict()\n",
    "config.optimizer.name = 'AdamW'\n",
    "config.optimizer.lr = 5e-4\n",
    "config.optimizer.betas = (0.9, 0.98)\n",
    "config.optimizer.weight_decay = 1e-4\n",
    "config.optimizer.eps = 1e-9\n",
    "config.scheduler = config_dict.ConfigDict()\n",
    "config.scheduler.name = 'CosineAnnealingLR'\n",
    "config.scheduler.T_max = 100\n",
    "\n",
    "model = SequenceRegressor(\n",
    "    input_size=config.input_size,\n",
    "    featurizer_args=config.featurizer,\n",
    "    rnn_args=config.rnn,\n",
    "    flows_args=config.flows,\n",
    "    optimizer_args=config.optimizer,\n",
    "    scheduler_args=config.scheduler,\n",
    "    sum_features=config.sum_features,\n",
    "    d_time = config.d_time,\n",
    "    d_time_projection = config.d_time_projection,\n",
    "    d_feat_projection = config.d_feat_projection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, norm_dict = prepare_dataloader(data, batch_size=32)\n",
    "batch = next(iter(train_loader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
